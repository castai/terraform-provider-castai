---
page_title: "castai_workload_scaling_policy Resource - terraform-provider-castai"
subcategory: ""
description: |-
  Manage workload scaling policy. Scaling policy reference https://docs.cast.ai/docs/woop-scaling-policies
---

# castai_workload_scaling_policy (Resource)

Manage workload scaling policy. Scaling policy [reference](https://docs.cast.ai/docs/woop-scaling-policies)

Scaling policies allow you to manage all your workloads centrally. You can apply the same settings to multiple workloads
simultaneously or create custom policies with different settings and apply them to multiple workloads.

## Example Usage

```terraform
resource "castai_workload_scaling_policy" "services" {
  name              = "services"
  cluster_id        = castai_gke_cluster.dev.id
  apply_type        = "IMMEDIATE"
  management_option = "MANAGED"
  assignment_rules {
    rules {
      namespace {
        names = ["default", "kube-system"]
      }
    }
    rules {
      workload {
        gvk = ["Deployment", "StatefulSet"]
        labels_expressions {
          key      = "region"
          operator = "NotIn"
          values   = ["eu-west-1", "eu-west-2"]
        }
        labels_expressions {
          key      = "helm.sh/chart"
          operator = "Exists"
        }
      }
    }
  }
  cpu {
    function = "QUANTILE"
    overhead = 0.15
    apply_threshold_strategy {
      type       = "PERCENTAGE"
      percentage = 0.1
    }
    args                     = ["0.9"]
    look_back_period_seconds = 172800
    min                      = 0.1
    max                      = 1
  }
  memory {
    function = "MAX"
    overhead = 0.35
    apply_threshold_strategy {
      type = "DEFAULT_ADAPTIVE"
    }
    limit {
      type       = "MULTIPLIER"
      multiplier = 1.5
    }
    management_option = "READ_ONLY"
  }
  startup {
    period_seconds = 240
  }
  downscaling {
    apply_type = "DEFERRED"
  }
  memory_event {
    apply_type = "IMMEDIATE"
  }
  anti_affinity {
    consider_anti_affinity = false
  }
  confidence {
    threshold = 0.9
  }
}
```

<!-- schema generated by tfplugindocs -->
## Schema

### Required

- `apply_type` (String) Recommendation apply type.
	- IMMEDIATE - pods are restarted immediately when new recommendation is generated.
	- DEFERRED - pods are not restarted and recommendation values are applied during natural restarts only (new deployment, etc.)
- `cluster_id` (String) CAST AI cluster id
- `cpu` (Block List, Min: 1, Max: 1) (see [below for nested schema](#nestedblock--cpu))
- `management_option` (String) Defines possible options for workload management.
	- READ_ONLY - workload watched (metrics collected), but no actions performed by CAST AI.
	- MANAGED - workload watched (metrics collected), CAST AI may perform actions on the workload.
- `memory` (Block List, Min: 1, Max: 1) (see [below for nested schema](#nestedblock--memory))
- `name` (String) Scaling policy name

### Optional

- `anti_affinity` (Block List, Max: 1) (see [below for nested schema](#nestedblock--anti_affinity))
- `assignment_rules` (Block List) Allows defining conditions for automatically assigning workloads to this scaling policy. (see [below for nested schema](#nestedblock--assignment_rules))
- `confidence` (Block List, Max: 1) Defines the confidence settings for applying recommendations. (see [below for nested schema](#nestedblock--confidence))
- `downscaling` (Block List, Max: 1) (see [below for nested schema](#nestedblock--downscaling))
- `memory_event` (Block List, Max: 1) (see [below for nested schema](#nestedblock--memory_event))
- `startup` (Block List, Max: 1) (see [below for nested schema](#nestedblock--startup))
- `timeouts` (Block, Optional) (see [below for nested schema](#nestedblock--timeouts))

### Read-Only

- `id` (String) The ID of this resource.

<a id="nestedblock--cpu"></a>
### Nested Schema for `cpu`

Optional:

- `apply_threshold` (Number, Deprecated) The threshold of when to apply the recommendation. Recommendation will be applied when diff of current requests and new recommendation is greater than set value
- `apply_threshold_strategy` (Block List, Max: 1) Resource apply threshold strategy settings. The default strategy is `PERCENTAGE` with percentage value set to 0.1. (see [below for nested schema](#nestedblock--cpu--apply_threshold_strategy))
- `args` (List of String) The arguments for the function - i.e. for `QUANTILE` this should be a [0, 1] float. `MAX` doesn't accept any args
- `function` (String) The function used to calculate the resource recommendation. Supported values: `QUANTILE`, `MAX`
- `limit` (Block List, Max: 1) Resource limit settings (see [below for nested schema](#nestedblock--cpu--limit))
- `look_back_period_seconds` (Number) The look back period in seconds for the recommendation.
- `management_option` (String) Disables management for a single resource when set to `READ_ONLY`. The resource will use its original workload template requests and limits. Supported value: `READ_ONLY`. Minimum required workload-autoscaler version: `v0.23.1`.
- `max` (Number) Max values for the recommendation, applies to every container. For memory - this is in MiB, for CPU - this is in cores.
- `min` (Number) Min values for the recommendation, applies to every container. For memory - this is in MiB, for CPU - this is in cores.
- `overhead` (Number) Overhead for the recommendation, e.g. `0.1` will result in 10% higher recommendation

<a id="nestedblock--cpu--apply_threshold_strategy"></a>
### Nested Schema for `cpu.apply_threshold_strategy`

Required:

- `type` (String) Defines apply theshold strategy type.
	- PERCENTAGE - recommendation will be applied when diff of current requests and new recommendation is greater than set value
    - DEFAULT_ADAPTIVE - will pick larger threshold percentage for small workloads and smaller percentage for large workloads.
    - CUSTOM_ADAPTIVE - works in same way as DEFAULT_ADAPTIVE, but it allows to tweak parameters of adaptive threshold formula: percentage = numerator/(currentRequest + denominator)^exponent. This strategy is for advance use cases, we recommend to use DEFAULT_ADAPTIVE strategy.

Optional:

- `denominator` (String) If denominator is close or equal to 0, the threshold will be much bigger for small values.For example when numerator, exponent is 1 and denominator is 0 the threshold for 0.5 req. CPU will be 200%.It must be defined for the CUSTOM_ADAPTIVE strategy.
- `exponent` (Number) The exponent changes how fast the curve is going down. The smaller value will cause that we won’t pick extremely small number for big resources, for example:
	- if numerator is 0, denominator is 1, and exponent is 1, for 50 CPU we will pick 2% threshold
	- if numerator is 0, denominator is 1, and exponent is 0.8, for 50 CPU we will pick 4.3% threshold
	It must be defined for the CUSTOM_ADAPTIVE strategy.
- `numerator` (Number) The numerator affects vertical stretch of function used in adaptive threshold - smaller number will create smaller threshold.It must be defined for the CUSTOM_ADAPTIVE strategy.
- `percentage` (Number) Percentage of a how much difference should there be between the current pod requests and the new recommendation. It must be defined for the PERCENTAGE strategy.


<a id="nestedblock--cpu--limit"></a>
### Nested Schema for `cpu.limit`

Required:

- `type` (String) Defines limit strategy type.
	- NO_LIMIT - removes the resource limit even if it was specified in the workload spec.
	- MULTIPLIER - used to calculate the resource limit. The final value is determined by multiplying the resource request by the specified factor.

Optional:

- `multiplier` (Number) Multiplier used to calculate the resource limit. It must be defined for the MULTIPLIER strategy.



<a id="nestedblock--memory"></a>
### Nested Schema for `memory`

Optional:

- `apply_threshold` (Number, Deprecated) The threshold of when to apply the recommendation. Recommendation will be applied when diff of current requests and new recommendation is greater than set value
- `apply_threshold_strategy` (Block List, Max: 1) Resource apply threshold strategy settings. The default strategy is `PERCENTAGE` with percentage value set to 0.1. (see [below for nested schema](#nestedblock--memory--apply_threshold_strategy))
- `args` (List of String) The arguments for the function - i.e. for `QUANTILE` this should be a [0, 1] float. `MAX` doesn't accept any args
- `function` (String) The function used to calculate the resource recommendation. Supported values: `QUANTILE`, `MAX`
- `limit` (Block List, Max: 1) Resource limit settings (see [below for nested schema](#nestedblock--memory--limit))
- `look_back_period_seconds` (Number) The look back period in seconds for the recommendation.
- `management_option` (String) Disables management for a single resource when set to `READ_ONLY`. The resource will use its original workload template requests and limits. Supported value: `READ_ONLY`. Minimum required workload-autoscaler version: `v0.23.1`.
- `max` (Number) Max values for the recommendation, applies to every container. For memory - this is in MiB, for CPU - this is in cores.
- `min` (Number) Min values for the recommendation, applies to every container. For memory - this is in MiB, for CPU - this is in cores.
- `overhead` (Number) Overhead for the recommendation, e.g. `0.1` will result in 10% higher recommendation

<a id="nestedblock--memory--apply_threshold_strategy"></a>
### Nested Schema for `memory.apply_threshold_strategy`

Required:

- `type` (String) Defines apply theshold strategy type.
	- PERCENTAGE - recommendation will be applied when diff of current requests and new recommendation is greater than set value
    - DEFAULT_ADAPTIVE - will pick larger threshold percentage for small workloads and smaller percentage for large workloads.
    - CUSTOM_ADAPTIVE - works in same way as DEFAULT_ADAPTIVE, but it allows to tweak parameters of adaptive threshold formula: percentage = numerator/(currentRequest + denominator)^exponent. This strategy is for advance use cases, we recommend to use DEFAULT_ADAPTIVE strategy.

Optional:

- `denominator` (String) If denominator is close or equal to 0, the threshold will be much bigger for small values.For example when numerator, exponent is 1 and denominator is 0 the threshold for 0.5 req. CPU will be 200%.It must be defined for the CUSTOM_ADAPTIVE strategy.
- `exponent` (Number) The exponent changes how fast the curve is going down. The smaller value will cause that we won’t pick extremely small number for big resources, for example:
	- if numerator is 0, denominator is 1, and exponent is 1, for 50 CPU we will pick 2% threshold
	- if numerator is 0, denominator is 1, and exponent is 0.8, for 50 CPU we will pick 4.3% threshold
	It must be defined for the CUSTOM_ADAPTIVE strategy.
- `numerator` (Number) The numerator affects vertical stretch of function used in adaptive threshold - smaller number will create smaller threshold.It must be defined for the CUSTOM_ADAPTIVE strategy.
- `percentage` (Number) Percentage of a how much difference should there be between the current pod requests and the new recommendation. It must be defined for the PERCENTAGE strategy.


<a id="nestedblock--memory--limit"></a>
### Nested Schema for `memory.limit`

Required:

- `type` (String) Defines limit strategy type.
	- NO_LIMIT - removes the resource limit even if it was specified in the workload spec.
	- MULTIPLIER - used to calculate the resource limit. The final value is determined by multiplying the resource request by the specified factor.

Optional:

- `multiplier` (Number) Multiplier used to calculate the resource limit. It must be defined for the MULTIPLIER strategy.



<a id="nestedblock--anti_affinity"></a>
### Nested Schema for `anti_affinity`

Optional:

- `consider_anti_affinity` (Boolean) Defines if anti-affinity should be considered when scaling the workload.
	If enabled, requiring host ports, or having anti-affinity on hostname will force all recommendations to be deferred.


<a id="nestedblock--assignment_rules"></a>
### Nested Schema for `assignment_rules`

Required:

- `rules` (Block List, Min: 1) (see [below for nested schema](#nestedblock--assignment_rules--rules))

<a id="nestedblock--assignment_rules--rules"></a>
### Nested Schema for `assignment_rules.rules`

Optional:

- `namespace` (Block List, Max: 1) Allows assigning a scaling policy based on the workload's namespace. (see [below for nested schema](#nestedblock--assignment_rules--rules--namespace))
- `workload` (Block List, Max: 1) Allows assigning a scaling policy based on the workload's metadata. (see [below for nested schema](#nestedblock--assignment_rules--rules--workload))

<a id="nestedblock--assignment_rules--rules--namespace"></a>
### Nested Schema for `assignment_rules.rules.namespace`

Optional:

- `names` (List of String) Defines matching by namespace names.


<a id="nestedblock--assignment_rules--rules--workload"></a>
### Nested Schema for `assignment_rules.rules.workload`

Optional:

- `gvk` (List of String) Group, version, and kind for Kubernetes resources. Format: kind[.version][.group].
It can be either:
 - only kind, e.g. "Deployment"
 - group and kind: e.g."Deployment.apps"
 - group, version and kind: e.g."Deployment.v1.apps"
- `labels_expressions` (Block List) Defines matching by label selector requirements. (see [below for nested schema](#nestedblock--assignment_rules--rules--workload--labels_expressions))

<a id="nestedblock--assignment_rules--rules--workload--labels_expressions"></a>
### Nested Schema for `assignment_rules.rules.workload.labels_expressions`

Required:

- `key` (String) The label key to match.
- `operator` (String) The operator to use for matching the label.

Optional:

- `values` (List of String) A list of values to match against the label key. Allowed for `In` and `NotIn` operators.





<a id="nestedblock--confidence"></a>
### Nested Schema for `confidence`

Optional:

- `threshold` (Number) Defines the confidence threshold for applying recommendations. The smaller number indicates that we require fewer metrics data points to apply recommendations - changing this value can cause applying less precise recommendations. Do not change the default unless you want to optimize with fewer data points (e.g., short-lived workloads).


<a id="nestedblock--downscaling"></a>
### Nested Schema for `downscaling`

Optional:

- `apply_type` (String) Defines the apply type to be used when downscaling.
	- IMMEDIATE - pods are restarted immediately when new recommendation is generated.
	- DEFERRED - pods are not restarted and recommendation values are applied during natural restarts only (new deployment, etc.)


<a id="nestedblock--memory_event"></a>
### Nested Schema for `memory_event`

Optional:

- `apply_type` (String) Defines the apply type to be used when applying recommendation for memory related event.
	- IMMEDIATE - pods are restarted immediately when new recommendation is generated.
	- DEFERRED - pods are not restarted and recommendation values are applied during natural restarts only (new deployment, etc.)


<a id="nestedblock--startup"></a>
### Nested Schema for `startup`

Optional:

- `period_seconds` (Number) Defines the duration (in seconds) during which elevated resource usage is expected at startup.
When set, recommendations will be adjusted to disregard resource spikes within this period.
If not specified, the workload will receive standard recommendations without startup considerations.


<a id="nestedblock--timeouts"></a>
### Nested Schema for `timeouts`

Optional:

- `create` (String)
- `delete` (String)
- `read` (String)
- `update` (String)

## Importing

For each connected cluster, a default scaling policy is created. An existing scaling policy can be imported into the
Terraform state using the `terraform import` command or the [`import`](https://developer.hashicorp.com/terraform/language/import#syntax) block (recommended for Terraform 1.5.0+).

Using the `import` block is a simpler and more convenient way of importing resources.

### Import using `import` block

#### Import a single scaling policy

1. Create an `import.tf` file with the following content:
   ```tf
   import {
   	 to       = castai_workload_scaling_policy.default
   	 id       = "<cluster_id>/<policy_id_or_name>" # e.g. "ff4c2211-3511-4d95-b6de-2919fc3287a3/default"
   }
   ```

2. Run the `terraform import` command:

   ```shell
   terraform plan -out=import.plan -var-file=tf.vars -generate-config-out=generated.tf
   ```

3. Review the `generated.tf` file and ensure the imported scaling policy is correct. Terraform will generate this file by setting values equal to zero for certain configuration parameters.

   For example:

   ```hcl
   cpu {
     look_back_period_seconds = 0
   }

4. Apply the import plan:

   ```shell
   terraform apply "import.plan"
   ```

#### Import multiple scaling policies

To import multiple scaling policies, you need to know the cluster IDs and the policy names. The `for_each` cannot be
used when generating configuration. As a result, you need to define the policy properties yourself, or you
can [import a single policy](#import-a-single-scaling-policy) and then use it as a template for other policies.

> [!NOTE]
> The below example assumes that you want to import the "default" scaling policy for multiple clusters. If you want to
> import
> scaling policies with different names, you need to adjust the `id` parameter in the `import` block accordingly.

1. Create the `import.tf` file with the following content:

   ```tf
   locals {
   	 policies = {
   		 "<cluster_name>" = "<cluster_id>"
   		 "<cluster_name>" = "<cluster_id>"
   		 "<cluster_name>" = "<cluster_id>"
   	 }
   }

   import {
   	 for_each = local.policies
   	 to       = castai_workload_scaling_policy.default[each.key]
   	 id       = "${each.value}/default"
   }

   resource "castai_workload_scaling_policy" "default" {
   	 for_each          = local.policies
   	 cluster_id        = each.value
   	 apply_type        = "IMMEDIATE"
   	 management_option = "READ_ONLY"
   	 name              = "default"
   	 cpu {
   		 apply_threshold          = 0.1
   		 args = ["0.80"]
   		 function                 = "QUANTILE"
   		 look_back_period_seconds = 86400
   		 min                      = 0.01
   	 }
   	 memory {
   		 apply_threshold          = 0.1
   		 args = []
   		 function                 = "MAX"
   		 look_back_period_seconds = 86400
   		 min                      = 10
   		 overhead                 = 0.1
   	 }
   }
   ```

2. Run the `terraform import` command and review the import plan:

   ```shell
   terraform plan -out=import.plan -var-file=tf.vars
   ```

3. Apply the import plan:

   ```shell
   terraform apply "import.plan"
   ```

### Import using the `terraform import` command

You can use the `terraform import` command to import existing scaling policy to Terraform state.

To import a resource, first write a resource block for it in your configuration, establishing the name by which
it will be known to Terraform:

```hcl
resource "castai_workload_scaling_policy" "services" {
  # ...
}
```

Now terraform import can be run to attach an existing scaling policy to this resource:
```shell
$ terraform import castai_workload_scaling_policy.services <cluster_id>/services
```

If you are using CAST AI Terraform modules, import command will be slightly different:
```shell
$ terraform import 'module.castai-eks-cluster.castai_workload_scaling_policy.this["services"]' <cluster_id>/services
```

## Upsert scaling policy

The recommended way is to [import](#importing) the scaling policy and then apply the changes to the policy.
However, if that’s not possible, you can define the default policy resource yourself. The CAST AI Terraform provider
will update the existing policy instead of returning an error.
